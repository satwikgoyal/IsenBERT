{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPe/xvT4rbryb1uZDY/HmQ5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HWnsbo9Osc8X","outputId":"83bc5c2a-69b7-453a-936c-5db0a9754b30"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"]},{"output_type":"stream","name":"stdout","text":["Failed to finish task in epsoide 0\n","now epsilon is 1, the reward is -200.0 maxPosition is -0.44190371063028894\n","Failed to finish task in epsoide 1\n","now epsilon is 0.95, the reward is -200.0 maxPosition is -0.3610588999702045\n","Failed to finish task in epsoide 2\n","now epsilon is 0.8999999999999999, the reward is -200.0 maxPosition is -0.40468136339539384\n","Failed to finish task in epsoide 3\n","now epsilon is 0.8499999999999999, the reward is -200.0 maxPosition is -0.37974243381622813\n","Failed to finish task in epsoide 4\n","now epsilon is 0.7999999999999998, the reward is -200.0 maxPosition is -0.439485492553997\n","Failed to finish task in epsoide 5\n","now epsilon is 0.7499999999999998, the reward is -200.0 maxPosition is -0.3713908064902077\n","Failed to finish task in epsoide 6\n","now epsilon is 0.6999999999999997, the reward is -200.0 maxPosition is -0.3637695276943275\n","Failed to finish task in epsoide 7\n","now epsilon is 0.6499999999999997, the reward is -200.0 maxPosition is -0.3349095982375451\n","Failed to finish task in epsoide 8\n","now epsilon is 0.5999999999999996, the reward is -200.0 maxPosition is -0.34383495261516994\n","Failed to finish task in epsoide 9\n","now epsilon is 0.5499999999999996, the reward is -200.0 maxPosition is -0.30864197940283\n","Failed to finish task in epsoide 10\n","now epsilon is 0.4999999999999996, the reward is -200.0 maxPosition is -0.2961638057150233\n","Failed to finish task in epsoide 11\n","now epsilon is 0.4499999999999996, the reward is -200.0 maxPosition is -0.3381172534430898\n","Failed to finish task in epsoide 12\n","now epsilon is 0.39999999999999963, the reward is -200.0 maxPosition is 0.09536159718716787\n","Failed to finish task in epsoide 13\n","now epsilon is 0.34999999999999964, the reward is -200.0 maxPosition is -0.07144383262521216\n","Failed to finish task in epsoide 14\n","now epsilon is 0.29999999999999966, the reward is -200.0 maxPosition is -0.23587657356051794\n","Failed to finish task in epsoide 15\n","now epsilon is 0.24999999999999967, the reward is -200.0 maxPosition is -0.2694862270536236\n","Failed to finish task in epsoide 16\n","now epsilon is 0.19999999999999968, the reward is -200.0 maxPosition is -0.2877651901047361\n"]}],"source":["% tensorflow_version 2.x\n","import tensorflow as tf\n","import keras\n","config = tf.compat.v1.ConfigProto( device_count = {'GPU': 2} ) \n","sess = tf.compat.v1.Session(config=config) \n","keras.backend.set_session(sess)\n","\n","import gym\n","from keras import models\n","from keras import layers\n","from tensorflow.keras.optimizers import Adam\n","from collections import deque\n","import random\n","import numpy as np\n","\n","\n","# device_name = tf.test.gpu_device_name()\n","# if device_name != '/device:GPU:0':\n","#   raise SystemError('GPU device not found')\n","# print('Found GPU at: {}'.format(device_name))\n","\n","\n","class DQN:\n","    def __init__(self,env):\n","        # parameters for RL\n","        self.env=env\n","        self.gamma=0.99\n","        self.epsilon = 1\n","        self.epsilon_decay = 0.05\n","        self.epsilon_min=0.01\n","        self.learning_rate=0.001\n","        self.episode_num=400\n","        self.steps_over_one_training = []\n","        self.steps_over_episodes = []\n","\n","        # parameters for Neural Network training\n","        self.timeout_steps=200 #max is 200\n","        self.replay_buffer_size=deque(maxlen=20000)\n","        self.num_selected_from_replay_buffer=32\n","        self.trainNetwork=self.createNetwork()\n","        self.targetNetwork=self.createNetwork()\n","        self.targetNetwork.set_weights(self.trainNetwork.get_weights())\n","        \n","\n","    # create the Keras neural network\n","    def createNetwork(self):\n","      # with tf.device('/device:GPU:0'):\n","\n","        # construct fully-connected neural network layer      \n","        model = models.Sequential()\n","        state_shape = self.env.observation_space.shape\n","        model.add(layers.Dense(24, activation='relu', input_shape=state_shape))\n","        model.add(layers.Dense(48, activation='relu'))\n","        model.add(layers.Dense(self.env.action_space.n,activation='linear'))\n","        # model.compile(optimizer=optimizers.RMSprop(lr=self.learning_rate), loss=losses.mean_squared_error)\n","        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n","        return model\n","\n","    # choose action at a certain state based on greedy \n","    def chooseAction(self,state):\n","\n","        self.epsilon = max(self.epsilon_min, self.epsilon)\n","\n","        if np.random.rand(1) < self.epsilon:\n","            action = np.random.randint(0, 3)\n","        else:\n","            action=np.argmax(self.trainNetwork.predict(state)[0])\n","\n","        return action\n","\n","    \n","    # use matrix to speed-up training\n","    def trainUsingReplayMemory_Boost(self):\n","        # with tf.device('/device:GPU:0'):\n","          if len(self.replay_buffer_size) < self.num_selected_from_replay_buffer:\n","              return\n","          samples = random.sample(self.replay_buffer_size,self.num_selected_from_replay_buffer)\n","          npsamples = np.array(samples)\n","          states_temp, actions_temp, rewards_temp, nextstates_temp, dones_temp = np.hsplit(npsamples, 5)\n","          states = np.concatenate((np.squeeze(states_temp[:])), axis = 0)\n","          rewards = rewards_temp.reshape(self.num_selected_from_replay_buffer,).astype(float)\n","          targets = self.trainNetwork.predict(states)\n","          newstates = np.concatenate(np.concatenate(nextstates_temp))\n","          dones = np.concatenate(dones_temp).astype(bool)\n","          notdones = ~dones\n","          notdones = notdones.astype(float)\n","          dones = dones.astype(float)\n","          Q_futures = self.targetNetwork.predict(newstates).max(axis = 1)\n","          targets[(np.arange(self.num_selected_from_replay_buffer), actions_temp.reshape(self.num_selected_from_replay_buffer,).astype(int))] = rewards * dones + (rewards + Q_futures * self.gamma)*notdones\n","          self.trainNetwork.fit(states, targets, epochs=1, verbose=0)\n","\n","\n","    # using replay memory for training, to avoid overshot\n","    def trainUsingReplayMemory(self):\n","\n","        # check if we have got enough data to train in the buffer\n","        # we need to get at least \"self.num_selected_from_replay_buffer\" number of data\n","        if len(self.replay_buffer_size) < self.num_selected_from_replay_buffer:\n","            return\n","\n","        # do the sampling\n","        samples = random.sample(self.replay_buffer_size,self.num_selected_from_replay_buffer)\n","\n","        # do the prediction for the target network\n","        states_list = []\n","        newStates_list=[]\n","        for sample in samples:\n","            state, action, reward, new_state, done = sample\n","            states_list.append(state)\n","            newStates_list.append(new_state)\n","\n","        newArray = np.array(states_list)\n","        states_list = newArray.reshape(self.num_selected_from_replay_buffer, 2)\n","\n","        newArray2 = np.array(newStates_list)\n","        newStates_list = newArray2.reshape(self.num_selected_from_replay_buffer, 2)\n","\n","        targets_for_train = self.trainNetwork.predict(states_list)\n","        new_state_targets=self.targetNetwork.predict(newStates_list)\n","\n","        i=0\n","        for sample in samples:\n","            state, action, reward, new_state, done = sample\n","            target = targets_for_train[i]\n","            if done:\n","                target[action] = reward\n","            else:\n","                Q_future = max(new_state_targets[i])\n","                target[action] = reward + Q_future * self.gamma\n","            i+=1\n","\n","        self.trainNetwork.fit(states_list, targets_for_train, epochs=1, verbose=0)\n","\n","\n","    def trainNN(self,currentState,eps):\n","        rewardSum = 0\n","        max_position=-99\n","\n","        for i in range(self.timeout_steps):\n","            bestAction = self.chooseAction(currentState)\n","\n","            new_state, reward, done, _ = env.step(bestAction)\n","\n","            new_state = new_state.reshape(1, 2)\n","\n","            # # Keep track of max position\n","            if new_state[0][0] > max_position:\n","                max_position = new_state[0][0]\n","\n","\n","            # # Adjust reward for task completion\n","            if new_state[0][0] >= 0.5:\n","                reward += 10\n","\n","            self.replay_buffer_size.append([currentState, bestAction, reward, new_state, done])\n","\n","            # this should be work when training on GPU\n","            self.trainUsingReplayMemory_Boost()\n","\n","            rewardSum += reward\n","\n","            currentState = new_state\n","\n","            if done:\n","                break\n","        if i >= 199:\n","            print(\"Failed to finish task in epsoide {}\".format(eps))\n","            \n","        else:\n","            print(\"Success in epsoide {}, used {} iterations!\".format(eps, i))\n","            self.trainNetwork.save('./trainNetworkInEPS{}.h5'.format(eps))\n","        self.steps_over_episodes.append(i)\n","\n","        #Sync\n","        self.targetNetwork.set_weights(self.trainNetwork.get_weights())\n","\n","        print(\"now epsilon is {}, the reward is {} maxPosition is {}\".format(max(self.epsilon_min, self.epsilon), rewardSum,max_position))\n","        self.epsilon -= self.epsilon_decay\n","    \n","    def testNN(self, N):\n","      model=models.load_model('trainNetworkInEPS399.h5')\n","\n","      # repeat the model with  times\n","      for i in range(N):\n","          curr_state = env.reset().reshape(1, 2)\n","          rewardSum=0\n","          steps_for_one_episode = []\n","\n","          # 200 steps maximum for one episode\n","          for t in range(self.timeout_steps):\n","              env.render()\n","              action = np.argmax(model.predict(curr_state)[0])\n","\n","              next_state, reward, done, info = env.step(action)\n","\n","              next_state = next_state.reshape(1, 2)\n","\n","              curr_state=next_state\n","\n","              rewardSum+=reward\n","\n","              steps += 1\n","              if done:\n","                  steps_for_one_episode.append()\n","                  print(\"Episode finished after {} timesteps reward is {}\".format(t+1,rewardSum))\n","                  break\n","        \n","\n","    # draw the result of N training times\n","    def plot_graphs(self, env, episodes, runs, params):\n","        with open(\"result_from_colab.txt\", \"r\") as f:\n","            all_lines = f.readlines()\n","            all_epoch_result = []\n","            all_episodes_result = []\n","            count = 0\n","            for line in all_lines:\n","                words_list = line.split(\" \")\n","                count += 1\n","                if words_list[0] == \"now\":\n","                    word = words_list[7]\n","                    num = -float(word)\n","                    if  len(all_episodes_result) < 400:\n","                        all_episodes_result.append(num)\n","                    elif len(all_episodes_result) >= 400:\n","                        print(\"current length is: \", len(all_episodes_result))\n","                        all_epoch_result.append(all_episodes_result.copy())\n","                        all_episodes_result = []\n","                        all_episodes_result.append(num)\n","            all_epoch_result.append(all_episodes_result)\n","           \n","            all_epochs_result_array = np.array(all_epoch_result)\n","            data_mean = np.mean(all_epochs_result_array, axis = 0)\n","            data_std = np.std(all_epochs_result_array, axis= 0)\n","            plt.plot(data_mean, 'g--')\n","            plt.fill_between(np.arange(len(data_mean)), data_mean - data_std, data_mean + data_std)\n","            plt.xlabel(\"Number of Episodes\")\n","            plt.ylabel(\"Number of Steps to Goal\")\n","            plt.title(\"Deep Q Learning between episodes and step to goal\")\n","            plt.show()\n","        return \n","\n","    def train(self, env, N, params):\n","      # Train the network for 5 times, each time with 500 episodes\n","      for i in range(N):\n","        self.steps_over_episodes = []\n","        for eps in range(self.episode_num):\n","            currentState=env.reset().reshape(1,2)\n","            self.trainNN(currentState, eps)\n","        self.steps_over_one_training.append(self.steps_over_episodes)\n","\n","      with open(\"result_from_colab.txt\", 'a') as f:\n","        f.write(self.steps_over_one_training)\n","\n","    def run_game(self, final_params):\n","\n","       return\n","\n","    def setGamma(self, gamma):\n","        self.gamma = gamma\n","        return\n","\n","    def setLearningrate(self, learning_rate):\n","        self.learning_rate = learning_rate\n","        return\n","\n","    def setEpsilon(self, epsilon):\n","        self.epsilon = epsilon\n","        return\n","\n","if __name__ == \"__main__\":\n","    env = gym.make('MountainCar-v0')\n","    dqn = DQN(env)\n","    params = [0.9, 0.001, 0.1]\n","    dqn.train(env, 5, params)"]}]}